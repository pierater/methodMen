
---
title: "Determining text data relation to Hip-Hop artist lyrics"
author: "Benito Sanchez, Sammuel Villavicencio, Martin Almaraz"
date: "May 12, 2017"
output: html_document
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
```

In 2013, Robert Galbraith published the novel The Cuckoo's Calling. It was later discovered through linguistic analysis that Robert Galbraith was just a pen name used by JK Rowling, the author of Harry Potter. This analysis flows through a similar vein. We sought to predict the artist of a song based on the form of their lyrics.


The data was aquired from: 
https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics

## Data Acquisition
The data set of nearly 400,000 songs and lyrics was acquired through MetroLyrics and hosted by kaggle.
```{r}

# read csv
data = read.csv("lyrics.csv", stringsAsFactors = F)
library("caret")
library("stringr")
library(rpart)
library(rpart.plot)

# the data seems to be very large at first...
nrow(data)

# but it includes many collums that we do not need, and genres we do not care for
names(data)

```

## Preprocessing and data munging
We decided to focus on the two hip-hop artists with the most lyrics.
```{r}

# remove index, year, genre, and X as they will not help us
# since our data includes many genres of music, we will need to remove anything but Hip-Hop
data = data[data$genre == "Hip-Hop",]

# remove any unessesary columns in the dataframe
data$index = NULL
data$year = NULL
data$genre = NULL
data$X = NULL
data$song = NULL

# remove all but top two artists
top = names(head(sort(table(data$artist), decreasing=TRUE), 2))
artists = data$artist
data = data[is.element(artists, top),]
data = data[!(data$lyrics == ''),]

```


## Create helper functions for sorting and generating features from our data
The features we selected include the number of words per song, the number of commas per song, the number of unique words per song, the number of lines per song and the number of words per line.

We created functions to calculate these features and bind them to the dataframe.
```{r}

# get number of words per lyric
get_num_words_in_lyric = function(lyric) {
    return(sapply(gregexpr("\\W+", lyric), length) + 1)
}

# get number of commas per song
get_num_of_commas = function(lyric) {
    return (str_count(lyric, ','))
}

get_words = function(words)
{
    words = scan(text = words, what='character', quote='')
    words = gsub('[^a-zA-Z]*', '', words)
    words = tolower(words)
    words = words[!(words == '')]
}

# number of unique words used per song
get_num_unique_words = function(lyric) {
    return(length(table(get_words(lyric))))
}

# get number of lines per song
get_num_lines = function(lyric) {
    return(str_count(lyric, '\n') + 1)
}

# get number of words per line per song
get_words_per_line = function(lyric) {
    return(mean(sapply(unlist(strsplit(lyric, '\n', fixed=TRUE)), get_num_words_in_lyric)))
}

# add these new features to our data frame
create_data = function(data) {
    data$words_line = sapply(data[,"lyrics"], get_words_per_line)
    data$num_lines = sapply(data[,"lyrics"], get_num_lines)
    data$num_unique = sapply(data[,"lyrics"], get_num_unique_words)
    data$num_commas = sapply(data[,"lyrics"], get_num_of_commas)
    data$num_words_total = sapply(data[,"lyrics"], get_num_words_in_lyric)
    data$lyrics = NULL
    return(data)
}

data = create_data(data)
```

### Data Exploration
We compare different aspects of these two artists such as shear amount of data that we have on each.
This is vital in creating accurate models and features in that we have more data to train and test with.
```{r}
  par(mar=c(5.1, max(4.1, max(nchar(names(data)))/1.8), 4.1, 2.1))
  barplot(head(sort(table(data$artist), decreasing = T), 2), horiz = T, las = 1, xlab = "Number of songs per artist", ylab = "Artist", col = c("green4", "red4", "blue4", "orange4"), main = "Number of songs per top 2 artists")

```


### Comparing features sizes for top 2 artists
An interesting thing to note on the feature data is that Eminem tends to have
much larger values for each of his features.
```{r}
  
  eminem = colMeans(data[data$artist == "eminem", c("num_words_total", "num_unique", "words_line", "num_lines", "num_commas")])
  
par(mar=c(5.1, max(4.1, max(nchar(names(data)))/1.8), 4.1, 2.1))
  barplot(eminem, horiz = T, las = 1, col = c("green4", "red4", "blue4", "orange4", "pink4"), main = "Comparing feature sizes for Eminem")
  
  chrisBrown = colMeans(data[data$artist == "chris-brown", c("num_words_total", "num_unique", "words_line", "num_lines", "num_commas")])
  
  par(mar=c(5.1, max(4.1, max(nchar(names(data)))/1.8), 4.1, 2.1))
  barplot(chrisBrown, horiz = T, las = 1, col =  c("green4", "red4", "blue4", "orange4", "pink4"), main = "Comparing feature sizes for Chris Brown")
```

## Creating our models Knn and classification tree
```{r}
# split into training and test data with a 80/20 split respectively
# split-data function sourced from Dr. Bruns lin-regr-util.R
split_data = function(dat, frac=c(0.75, 0.25)) {
    k = length(frac)
    stopifnot(k > 0)

    n = nrow(dat)
    frac = frac/(sum(frac))
    starts = c(1, round(cumsum(frac) * n)[-k])
    ends = c(starts[-1]-1,n)
    samp = sample(1:n)
    data_sets = list()
    for(i in 1:k) {
        data_sets[[i]] = dat[samp[starts[i]:ends[i]],]
    }
    return(data_sets)
}

data_sets = split_data(data, c(0.80, 0.20))
tr_dat = data_sets[[1]]
test_dat = data_sets[[2]]

# creating models
x = split_data(data)
tr_dat = x[[1]]
te_dat = x[[2]]

tree = rpart(artist ~ ., data = tr_dat, method='class')

knn_model = train(artist ~ ., data = tr_dat, method="knn", tuneLength = 10)

```

## Evaluating the Knn model

Training tests on different number of k values and the k value with the best accuracy is chosen. Here we see that k = 12 gives the best accuracy.

The scatterplot representing the knn model shows us very interesting results also.
It looks like the boundary for Eminem vs Chris Brown is oddly overlapped. This could be because
we scale the 5 features that the knn model uses into 2, which leads to odd artifacts.
```{r}
plot(knn_model, main = "Accuray vs. Knn value")

d = dist(data)
fit = cmdscale(d)
plot(fit[data$artist == 'chris-brown'], col='orange', xlab='Property 1', ylab='Property 2', main='Songs by Eminem vs Chris Brown')
points(fit[data$artist == 'eminem'], col='green')
legend('topright', legend=c('Chris Brown', 'Eminem'), col=c('orange', 'green'), pch=1)


summary(knn_model)

predicted_knn = predict(knn_model, newdata=te_dat)
table(te_dat$artist, predicted_knn)

# Decent accuracy, we find approxamatly 83%
mean(te_dat$artist == predicted_knn)
```

## Evaluating the classification tree model
The tree model gives us very interesting data on how the two artists structure their songs.
It seems that the main difference between the two artists is that Eminem tends to use more unique words per song, more lines per song, more words, and basically more of all the features we have. 
This is very close to what our barplots showed us earlier in the report.
```{r}
prp(tree, extra=106, varlen=-10, main="Decide whether a song is by Eminem or Chris Brown", box.col=c("red4", "lightblue")[tree$frame$yval])

summary(tree)

pred = predict(tree, te_dat, type='class')
table(te_dat$artist, pred)

# The classification tree tended to work slightly better than the  knn model
mean(te_dat$artist == pred)
```

## Conclusion
Our final results show that when given a sample text, or unknown lyric, we can with good accuracy determine if it falls more as an Eminem song, or Chris Brown song. Furthermore when we try to adapt our models to work with the rest of the subset of artists, we find that our accuracy falls extremely poor. When classifying 5 or more artists, we find that accuracy falls into the single digit percentile, around 6%. 

## Further exploration
In order to improve our accuracy for more than just two artist classes, we believe we must find much better features to examine. What may help us is speaking to a linguist, or someone generally more knowledgeable than us who can identify better ways to breakdown lyrics.
